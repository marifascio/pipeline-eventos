# ğŸ“Š Pipeline de Eventos com Airflow e BigQuery  

Este projeto automatiza a coleta, transformaÃ§Ã£o e anÃ¡lise de eventos do site utilizando **Apache Airflow**, **Google Cloud Storage (GCS)** e **BigQuery**.

---

## ğŸ“ Estrutura do Projeto

``` sh
pipeline-eventos/
â”‚â”€â”€ dags/                   # DAGs do Airflow
â”‚   â”œâ”€â”€ bq_pipeline_dag.py
â”‚   â”œâ”€â”€ etl_vendas.py
â”‚
â”‚â”€â”€ scripts/                # Scripts auxiliares
â”‚   â”œâ”€â”€ gerar_eventos.py
â”‚   â”œâ”€â”€ publicar_mensagens.py
â”‚
â”‚â”€â”€ data/                   # Arquivos de dados
â”‚   â”œâ”€â”€ vendas.csv
â”‚   â”œâ”€â”€ website_logs.csv
â”‚
â”‚â”€â”€ configs/                # ConfiguraÃ§Ãµes (NÃƒO COMMITAR)
â”‚
â”‚â”€â”€ .gitignore              # Arquivos ignorados pelo Git
â”‚â”€â”€ requirements.txt        # DependÃªncias do projeto
â”‚â”€â”€ README.md               # DocumentaÃ§Ã£o do projeto
```

---

## ğŸ”¥ Tecnologias Utilizadas

- **Apache Airflow** â€“ OrquestraÃ§Ã£o do pipeline  
- **Google Cloud Platform (GCP)** â€“ BigQuery e Cloud Storage  
- **Python** â€“ Scripts de processamento e ingestÃ£o  
- **Google Pub/Sub** â€“ Streaming de mensagens\

---

## ğŸ¯ Objetivo do Projeto

1. **Coletar dados de eventos do site** (CSV no GCS).  
2. **Ingerir os dados no BigQuery** automaticamente via Airflow.  
3. **Executar consultas analÃ­ticas** sobre acessos ao site.  
4. **Automatizar o fluxo de dados** e garantir escalabilidade.  

---

## âš™ï¸ Como Executar o Projeto

### 1ï¸âƒ£ **Configurar o Ambiente**
Clone este repositÃ³rio e crie um ambiente virtual:

```sh
git clone https://github.com/marifascio/pipeline-eventos.git
cd pipeline-eventos
python -m venv airflow_venv
source airflow_venv/bin/activate  # Linux/Mac
airflow_venv\Scripts\activate     # Windows
```

Instale as dependÃªncias:

```sh
pip install -r requirements.txt
```

### 2ï¸âƒ£ **Configurar o Airflow**
Inicialize o banco de dados:

```sh
airflow db init
```

Crie um usuÃ¡rio admin:

```sh
airflow users create \
    --username admin \
    --password admin \
    --firstname user \
    --lastname admin \
    --role Admin \
    --email example@mail.com
```

Inicie o Airflow:

```sh
airflow webserver --port 8080
airflow scheduler
```

Acesse http://localhost:8080 e ative a DAG bq_pipeline_dag.

## ğŸ“Š Exemplo de Query no BigQuery

``` sql
SELECT device_type, COUNT(*) AS total_acessos
FROM `meu-projeto-bigquery-452219.website_data.website_logs`
GROUP BY device_type
ORDER BY total_acessos DESC;
```

## âš ï¸ **Importante**
-  NÃƒO COMMITAR credenciais (chave.json, .env) para o GitHub.
-  Utilizar variÃ¡veis de ambiente para armazenar credenciais.
-  Configurar permissÃµes no GCP para execuÃ§Ã£o do pipeline.

## âœ¨ **Autora**
- ğŸ‘©â€ğŸ’» Mariana Fascio
- ğŸ“§ marianafascio@gmail.com
- ğŸŒ LinkedIn: **https://www.linkedin.com/in/marianafascio/**

